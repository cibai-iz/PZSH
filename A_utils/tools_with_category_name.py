import numpy as np
import torch.utils.data as util_data
from torchvision import transforms
import torch
from PIL import Image
from tqdm import tqdm
import torchvision.datasets as dsets
from torchvision.transforms.functional import InterpolationMode
from torch.cuda.amp import autocast  # è‡ªåŠ¨æ··åˆç²¾åº¦
import os

def config_dataset(config):
    if config["dataset"] in ["CUB", "CUB_add"]:
        config["topK"] = 1000  # 1000
        config["n_class"] = 200
    elif config["dataset"] == "AWA":
        config["topK"] = 4000  # 4000
        config["n_class"] = 50

    if config["dataset"] == "CUB":  # CUB_æ•´åˆç‰ˆï¼Œå…¶å®å›¾ç‰‡åº“å°±ç”¨åŠ äº†æ–‡ç”Ÿå›¾çš„å°±è¡Œï¼Œåªæ˜¯txtè¦å˜ï¼Œæ‰€ä»¥æŠŠåŸç‰ˆçš„train.txtä¹Ÿæ”¾åˆ°æ–‡ç”Ÿå›¾ç›®å½•é‡Œï¼Œè¿™æ ·å°±æ•´åˆäº†
          config["data_path"] = "dataset/CUB/CUB-last50_is_txt2img/images/"
    if config["dataset"] == "AWA":  # AWA
        config["data_path"] = "dataset/AWA/JPEGImages/"    

    if config["dataset"] == "CUB" :
        if config["TGI"] == 1:
            config["data"] = {
            # CUB_æ•´åˆç‰ˆ(å…¶å®åªæœ‰è®­ç»ƒé›†ä¸åŒï¼Œå…¶ä»–éƒ½ç›¸åŒçš„):
            "train_set" : {"list_path": f"dataset/CUB/CUB-last50_is_txt2img/images/train_40_with_caption_catgoryname_blip768F.txt", "batch_size": config["batch_size"]},

            
            "database": {"list_path": f"dataset/CUB/CUB-last50_is_txt2img/images/database1.txt", "batch_size": config["batch_size"]},
            "test": {"list_path": f"dataset/CUB/CUB-last50_is_txt2img/images/test1.txt", "batch_size": config["batch_size"]}
            }
        else:
            config["data"] = {
            # CUB_æ•´åˆç‰ˆ(å…¶å®åªæœ‰è®­ç»ƒé›†ä¸åŒï¼Œå…¶ä»–éƒ½ç›¸åŒçš„):
            "train_set" : {"list_path": f"dataset/CUB/CUB-last50_is_txt2img/images/train_40_with_caption_catgoryname_blip768F_NOTGI.txt", "batch_size": config["batch_size"]},

            "database": {"list_path": f"dataset/CUB/CUB-last50_is_txt2img/images/database1.txt", "batch_size": config["batch_size"]},
            "test": {"list_path": f"dataset/CUB/CUB-last50_is_txt2img/images/test1.txt", "batch_size": config["batch_size"]}
            }
    else:  # AWA
        if config["TGI"] == 1:
            config["data"] = {  # AWAçš„
                "train_set" : {"list_path": f"dataset/AWA/JPEGImages/train_100_with_caption_catgoryname_AttrVoc_mskimg_SDimg_blip768F.txt", "batch_size": config["batch_size"]},

                "database": {"list_path": f"dataset/AWA/filetxt/database.txt", "batch_size": config["batch_size"]},
                "test": {"list_path": f"dataset/AWA/filetxt/test.txt", "batch_size": config["batch_size"]}
                }
        else:
            config["data"] = {  # AWAçš„
                "train_set" : {"list_path": f"dataset/AWA/JPEGImages/train_100_with_caption_catgoryname_AttrVoc_mskimg_SDimg_blip768F_NOTGI.txt", "batch_size": config["batch_size"]},

                "database": {"list_path": f"dataset/AWA/filetxt/database.txt", "batch_size": config["batch_size"]},
                "test": {"list_path": f"dataset/AWA/filetxt/test.txt", "batch_size": config["batch_size"]}
                }
    return config



draw_range = [1, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500,
              9000, 9500, 10000]

def pr_curve(rF, qF, rL, qL, draw_range=draw_range):
    #  https://blog.csdn.net/HackerTom/article/details/89425729
    n_query = qF.shape[0]
    Gnd = (np.dot(qL, rL.transpose()) > 0).astype(np.float32)
    Rank = np.argsort(CalcHammingDist(qF, rF))
    P, R = [], []
    for k in tqdm(draw_range):
        p = np.zeros(n_query)
        r = np.zeros(n_query)
        for it in range(n_query):
            gnd = Gnd[it]
            gnd_all = np.sum(gnd)
            if gnd_all == 0:
                continue
            asc_id = Rank[it][:k]
            gnd = gnd[asc_id]
            gnd_r = np.sum(gnd)
            p[it] = gnd_r / k
            r[it] = gnd_r / gnd_all
        P.append(np.mean(p))
        R.append(np.mean(r))
    return P, R

class ImageList_for_train(object):  # æ–°å†™ä¸€ä¸ªï¼Œç”¨æ¥è¯»å¸¦æœ‰æ–‡æœ¬æè¿°çš„æ•°æ®é›†ã€‚
    # ImageList_for_train ç±»çš„æ–°å®ä¾‹æ—¶ï¼Œä¼šè°ƒç”¨è¿™ä¸ªå‡½æ•°ã€‚å®ƒæ¥å—ä¸‰ä¸ªå‚æ•°ï¼š 
    # data_pathï¼šåŒ…å«å›¾åƒæ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚
    # image_listï¼šä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«å›¾åƒæ–‡ä»¶çš„è·¯å¾„å’Œå¯¹åº”çš„æ ‡ç­¾ã€‚æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç©ºæ ¼åˆ†éš”ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å›¾åƒçš„ç›¸å¯¹è·¯å¾„ï¼Œå‰©ä¸‹çš„æ˜¯æ ‡ç­¾å€¼ã€‚
    # transformï¼šä¸€ä¸ªå‡½æ•°æˆ–è½¬æ¢å¯¹è±¡ï¼Œç”¨äºå¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼ˆå¦‚ç¼©æ”¾ã€å½’ä¸€åŒ–ç­‰ï¼‰ã€‚
    def __init__(self, data_path, image_list, transform_for_vae, transform_for_clip):
        self.imgs = [
            (
                data_path + val.split('\t')[0],  # å›¾åƒè·¯å¾„
                np.array([int(la) for la in val.split('\t')[1].split()]),  # æ ‡ç­¾å‘é‡

                # val.split('\t')[7],  # BLIPç‰¹å¾  AWAç”¨è¿™ä¸ª
                val.split('\t')[-1],  # BLIPç‰¹å¾  CUBç”¨è¿™ä¸ª
                # val.split('\t')[2],  # æ–‡æœ¬æè¿°
                # val.split('\t')[3]  # ç±»åˆ«å
            )
            for val in image_list
        ]
        # self.transform_for_vae = transform_for_vae()
        # self.transform_for_clip = transform_for_clip()
        self.transform_for_blip = image_transform_for_blip()

    def __getitem__(self, index):
        path, label_onehot, BLIP_target = self.imgs[index]
        img = Image.open(path).convert('RGB')
        # img_for_vae = self.transform_for_vae(img)
        # img_for_clip = self.transform_for_clip(img)
        img_for_blip = self.transform_for_blip(img)

        # ğŸ’¥ è¿™é‡ŒåŠ ï¼ï¼æŠŠBLIP_targetä»å­—ç¬¦ä¸²å˜æˆTensor
        BLIP_target = torch.tensor([float(x) for x in BLIP_target.strip().split()], dtype=torch.float)

        return img_for_blip, label_onehot, BLIP_target, index  # è¿”å›å›¾åƒã€æ ‡ç­¾ã€æè¿°å’Œç´¢å¼•

    def __len__(self):
        return len(self.imgs)

class ImageList(object):  # æ–°å†™ä¸€ä¸ªï¼Œç”¨æ¥è¯»å¸¦æœ‰æ–‡æœ¬æè¿°çš„æ•°æ®é›†ã€‚
    # ImageList ç±»çš„æ–°å®ä¾‹æ—¶ï¼Œä¼šè°ƒç”¨è¿™ä¸ªå‡½æ•°ã€‚å®ƒæ¥å—ä¸‰ä¸ªå‚æ•°ï¼š 
    # data_pathï¼šåŒ…å«å›¾åƒæ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚
    # image_listï¼šä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«å›¾åƒæ–‡ä»¶çš„è·¯å¾„å’Œå¯¹åº”çš„æ ‡ç­¾ã€‚æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç©ºæ ¼åˆ†éš”ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å›¾åƒçš„ç›¸å¯¹è·¯å¾„ï¼Œå‰©ä¸‹çš„æ˜¯æ ‡ç­¾å€¼ã€‚
    # transformï¼šä¸€ä¸ªå‡½æ•°æˆ–è½¬æ¢å¯¹è±¡ï¼Œç”¨äºå¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼ˆå¦‚ç¼©æ”¾ã€å½’ä¸€åŒ–ç­‰ï¼‰ã€‚
    def __init__(self, data_path, image_list, transform_for_vae=None, transform_for_clip=None):
        self.imgs = [
            (
                data_path + val.split('\t')[0],  # å›¾åƒè·¯å¾„
                np.array([int(la) for la in val.split('\t')[1].split()]),  # æ ‡ç­¾å‘é‡
                # val.split('\t')[7],  # BLIPç‰¹å¾
                # val.split('\t')[2],  # æ–‡æœ¬æè¿°
                # val.split('\t')[3]  # ç±»åˆ«å
            )
            for val in image_list
        ]
        # self.transform_for_vae = transform_for_vae()
        # self.transform_for_clip = transform_for_clip()
        self.transform_for_blip = image_transform_for_blip()

    def __getitem__(self, index):
        path, label_onehot = self.imgs[index]
        img = Image.open(path).convert('RGB')
        # img_for_vae = self.transform_for_vae(img)
        # img_for_clip = self.transform_for_clip(img)
        img_for_blip = self.transform_for_blip(img)
        return img_for_blip, label_onehot, index  # è¿”å›å›¾åƒã€æ ‡ç­¾ã€æè¿°å’Œç´¢å¼•

    def __len__(self):
        return len(self.imgs)
    
def image_transform_for_vae():
    # ç¡®ä¿å›¾åƒå¤§å°ç¬¦åˆ vae çš„è¾“å…¥è¦æ±‚
    return transforms.Compose([
       transforms.Resize((512, 512)),  # è°ƒæ•´å›¾åƒå¤§å°åˆ°512x512
        transforms.ToTensor(),          # å°† PIL Image è½¬ä¸ºå¼ é‡ï¼ŒèŒƒå›´ [0,1]
        transforms.Normalize(
            mean=(0.5, 0.5, 0.5),
            std=(0.5, 0.5, 0.5)  # ä½¿ç”¨å‡å€¼0.5å’Œæ ‡å‡†å·®0.5, å°†[0,1]èŒƒå›´å˜æ¢åˆ°[-1,1]
        )
    ])

def image_transform_for_clip():
    # ç¡®ä¿å›¾åƒå¤§å°ç¬¦åˆ CLIP çš„è¾“å…¥è¦æ±‚
    return transforms.Compose([
        transforms.Resize((224, 224)),  # è°ƒæ•´å›¾åƒå¤§å°åˆ°224x224
        transforms.ToTensor(),          # å°† PIL Image è½¬ä¸ºå¼ é‡ï¼ŒèŒƒå›´ [0,1]
        transforms.Normalize(
            mean=(0.48145466, 0.4578275, 0.40821073),
            std=(0.26862954, 0.26130258, 0.27577711)
        )
    ])
def image_transform_for_blip():
    return transforms.Compose([
        transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=(0.48145466, 0.4578275, 0.40821073),
            std=(0.26862954, 0.26130258, 0.27577711)
        )
    ])

class MyCIFAR10(dsets.CIFAR10):
    def __getitem__(self, index):
        img, target = self.data[index], self.targets[index]
        img = Image.fromarray(img)
        img = self.transform(img)
        target = np.eye(10, dtype=np.int8)[np.array(target)]
        return img, target, index


def cifar_dataset(config):
    batch_size = config["batch_size"]

    train_size = 500
    test_size = 100

    if config["dataset"] == "cifar10-2":
        train_size = 5000
        test_size = 1000

    transform = transforms.Compose([
        transforms.Resize(config["crop_size"]),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    cifar_dataset_root = 'dataset/cifar/'
    # Dataset
    train_dataset = MyCIFAR10(root=cifar_dataset_root,
                              train=True,
                              transform=transform,
                              download=True)

    test_dataset = MyCIFAR10(root=cifar_dataset_root,
                             train=False,
                             transform=transform)

    database_dataset = MyCIFAR10(root=cifar_dataset_root,
                                 train=False,
                                 transform=transform)

    X = np.concatenate((train_dataset.data, test_dataset.data))
    L = np.concatenate((np.array(train_dataset.targets), np.array(test_dataset.targets)))

    first = True
    for label in range(10):
        index = np.where(L == label)[0]

        N = index.shape[0]
        perm = np.random.permutation(N)
        index = index[perm]

        if first:
            test_index = index[:test_size]
            train_index = index[test_size: train_size + test_size]
            database_index = index[train_size + test_size:]
        else:
            test_index = np.concatenate((test_index, index[:test_size]))
            train_index = np.concatenate((train_index, index[test_size: train_size + test_size]))
            database_index = np.concatenate((database_index, index[train_size + test_size:]))
        first = False

    if config["dataset"] == "cifar10":
        # test:1000, train:5000, database:54000
        pass
    elif config["dataset"] == "cifar10-1":
        # test:1000, train:5000, database:59000
        database_index = np.concatenate((train_index, database_index))
    elif config["dataset"] == "cifar10-2":
        # test:10000, train:50000, database:50000
        database_index = train_index

    train_dataset.data = X[train_index]
    train_dataset.targets = L[train_index]
    test_dataset.data = X[test_index]
    test_dataset.targets = L[test_index]
    database_dataset.data = X[database_index]
    database_dataset.targets = L[database_index]

    print("train_dataset", train_dataset.data.shape[0])
    print("test_dataset", test_dataset.data.shape[0])
    print("database_dataset", database_dataset.data.shape[0])

    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                               batch_size=batch_size,
                                               shuffle=True,
                                               num_workers=4)

    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                              batch_size=batch_size,
                                              shuffle=False,
                                              num_workers=4)

    database_loader = torch.utils.data.DataLoader(dataset=database_dataset,
                                                  batch_size=batch_size,
                                                  shuffle=False,
                                                  num_workers=4)

    return train_loader, test_loader, database_loader, \
           train_index.shape[0], test_index.shape[0], database_index.shape[0]


def get_data(config):
    if "cifar" in config["dataset"]:
        return cifar_dataset(config)

    dsets = {}
    dset_loaders = {}
    data_config = config["data"] # å›¾ç‰‡è®­ç»ƒæµ‹è¯•æ£€ç´¢ä¸‰ç§å›¾ç‰‡é›†åˆçš„ä½ç½®

    for data_set in ["train_set"]:
        dsets[data_set] = ImageList_for_train(config["data_path"],
                                    open(data_config[data_set]["list_path"]).readlines(),
                                    transform_for_vae=image_transform_for_vae(),
                                    transform_for_clip=image_transform_for_clip())
        print(data_set, len(dsets[data_set]))
        dset_loaders[data_set] = util_data.DataLoader(dsets[data_set],
                                                    batch_size=data_config[data_set]["batch_size"],
                                                    shuffle=True, num_workers=4)
        
    for data_set in ["test", "database"]:
        dsets[data_set] = ImageList(config["data_path"],
                                    open(data_config[data_set]["list_path"]).readlines(),
                                    transform_for_vae=image_transform_for_vae(),
                                    transform_for_clip=image_transform_for_clip())
        print(data_set, len(dsets[data_set]))
        dset_loaders[data_set] = util_data.DataLoader(dsets[data_set],
                                                    batch_size=data_config[data_set]["batch_size"],
                                                    shuffle=False, num_workers=4)
    return dset_loaders["train_set"], dset_loaders["test"], dset_loaders["database"], \
        len(dsets["train_set"]), len(dsets["test"]), len(dsets["database"]) # è¿™é‡Œè¿”å›çš„å°±æ˜¯è®­ç»ƒï¼Œæµ‹è¯•ï¼Œæ£€ç´¢é›†çš„åˆ†åˆ«å›¾ç‰‡æ•°é‡



# ==================================== get_data_for_CLIP ====================================
class ImageList_for_train_CLip(object):
    def __init__(self, data_path, image_list, transform_for_clip):
        self.imgs = [
            (
                os.path.join(data_path, val.split('\t')[0]),  # å›¾åƒè·¯å¾„
                np.array([int(la) for la in val.split('\t')[1].split()]),  # æ ‡ç­¾ one-hot
                val.split('\t')[4]  # ç¬¬5åˆ—ä¸ºä¿å­˜çš„CLIPç‰¹å¾å­—ç¬¦ä¸²
            )
            for val in image_list
        ]
        self.transform_for_clip = transform_for_clip

    def __getitem__(self, index):
        path, label_onehot, clip_target = self.imgs[index]
        img = Image.open(path).convert('RGB')
        img_for_clip = self.transform_for_clip(img)
        clip_target = torch.tensor([float(x) for x in clip_target.strip().split()], dtype=torch.float)
        return img_for_clip, label_onehot, clip_target, index

    def __len__(self):
        return len(self.imgs)

class ImageList_for_Clip(object):
    def __init__(self, data_path, image_list, transform_for_clip):
        self.imgs = [
            (
                os.path.join(data_path, val.split('\t')[0]),
                np.array([int(la) for la in val.split('\t')[1].split()])
            )
            for val in image_list
        ]
        self.transform_for_clip = transform_for_clip

    def __getitem__(self, index):
        path, label_onehot = self.imgs[index]
        img = Image.open(path).convert('RGB')
        img_for_clip = self.transform_for_clip(img)
        return img_for_clip, label_onehot, index

    def __len__(self):
        return len(self.imgs)


def get_data_for_CLIP(config):
    if "cifar" in config["dataset"]:
        return cifar_dataset(config)

    dsets = {}
    dset_loaders = {}
    data_config = config["data"] # å›¾ç‰‡è®­ç»ƒæµ‹è¯•æ£€ç´¢ä¸‰ç§å›¾ç‰‡é›†åˆçš„ä½ç½®

    for data_set in ["train_set"]:
        dsets[data_set] = ImageList_for_train_CLip(config["data_path"],
                                    open(data_config[data_set]["list_path"]).readlines(),
                                    transform_for_clip=image_transform_for_clip())
        print(data_set, len(dsets[data_set]))
        dset_loaders[data_set] = util_data.DataLoader(dsets[data_set],
                                                    batch_size=data_config[data_set]["batch_size"],
                                                    shuffle=True, num_workers=4)
        
    for data_set in ["test", "database"]:
        dsets[data_set] = ImageList_for_Clip(config["data_path"],
                                    open(data_config[data_set]["list_path"]).readlines(),
                                    transform_for_clip=image_transform_for_clip())
        print(data_set, len(dsets[data_set]))
        dset_loaders[data_set] = util_data.DataLoader(dsets[data_set],
                                                    batch_size=data_config[data_set]["batch_size"],
                                                    shuffle=False, num_workers=4)
    return dset_loaders["train_set"], dset_loaders["test"], dset_loaders["database"], \
        len(dsets["train_set"]), len(dsets["test"]), len(dsets["database"]) # è¿™é‡Œè¿”å›çš„å°±æ˜¯è®­ç»ƒï¼Œæµ‹è¯•ï¼Œæ£€ç´¢é›†çš„åˆ†åˆ«å›¾ç‰‡æ•°é‡




# ==================================== è®¡ç®—ç²¾åº¦ ====================================
def compute_result(dataloader, net, device):
    bs, clses = [], []
    net.eval()
    for image_for_vae, image_for_clip, cls, ind in tqdm(dataloader):
        clses.append(cls)

        # output = net(img.to(device))   
        # print(output)  # æ‰“å°è¾“å‡ºå†…å®¹ï¼Œæ£€æŸ¥è¿”å›çš„å€¼
        hash_codes, _ = net(image_for_vae.to(device), image_for_clip.to(device)) 
        bs.append(hash_codes.data.cpu())    
        # bs.append((net(img.to(device))).data.cpu())
    return torch.cat(bs).sign(), torch.cat(clses)

def compute_result_BlipHash1(dataloader, net, device):
    bs, clses = [], []
    net.eval()
    with torch.no_grad():
        for images, labels, _ in tqdm(dataloader, desc="Computing hash codes"):
            clses.append(labels)
            images = images.to(device)

            with autocast():  # ğŸ’¥ å¼€å¯æ··åˆç²¾åº¦åŠ é€Ÿæ¨ç†ï¼ŒåŠ é€Ÿç¨‹åºè¿è¡Œæ•ˆç‡
                _, hash_codes, _ = net(images)
            # _, hash_codes = net(images)  # âœ…åªè¾“å…¥å›¾åƒï¼Œå–å‡ºå“ˆå¸Œå±‚è¾“å‡º
            bs.append(hash_codes.data.cpu())

    return torch.cat(bs).sign(), torch.cat(clses)

def compute_result_BlipHash_4_5(dataloader, net, device):
    bs, clses = [], []
    net.eval()
    with torch.no_grad():
        for images, labels, _ in tqdm(dataloader, desc="Computing hash codes"):
            clses.append(labels)
            images = images.to(device)

            with autocast():  # ğŸ’¥ å¼€å¯æ··åˆç²¾åº¦åŠ é€Ÿæ¨ç†ï¼ŒåŠ é€Ÿç¨‹åºè¿è¡Œæ•ˆç‡
                _, hash_codes, _, _ = net(images)
            # _, hash_codes = net(images)  # âœ…åªè¾“å…¥å›¾åƒï¼Œå–å‡ºå“ˆå¸Œå±‚è¾“å‡º
            bs.append(hash_codes.data.cpu())

    return torch.cat(bs).sign(), torch.cat(clses)

def compute_result_BlipHash2(dataloader, net, device):
    bs, clses = [], []
    net.eval()
    with torch.no_grad():
        for images, labels, _ in tqdm(dataloader, desc="Computing hash codes"):
            clses.append(labels)
            images = images.to(device)

            with autocast():  # ğŸ’¥ å¼€å¯æ··åˆç²¾åº¦åŠ é€Ÿæ¨ç†ï¼ŒåŠ é€Ÿç¨‹åºè¿è¡Œæ•ˆç‡
                _, hash_codes, _ = net(images)
            # _, hash_codes = net(images)  # âœ…åªè¾“å…¥å›¾åƒï¼Œå–å‡ºå“ˆå¸Œå±‚è¾“å‡º
            bs.append(hash_codes.data.cpu())

    return torch.cat(bs).sign(), torch.cat(clses)

def compute_result_with_caption(dataloader, net, device):
    bs, clses = [], []
    net.eval()
    for img, cls, _ in tqdm(dataloader):
        clses.append(cls)

        hash_codes, _ = net(img.to(device))
        bs.append(hash_codes.data.cpu())    
        # bs.append((net(img.to(device))).data.cpu())
    return torch.cat(bs).sign(), torch.cat(clses)

def compute_result_with_caption_imgandtxtloss(dataloader, net, device):
    bs, clses = [], []
    net.eval()
    for img, cls, _ in tqdm(dataloader):
        clses.append(cls)
        
        hash_codes, _, _ = net(img.to(device)) 
        bs.append(hash_codes.data.cpu())    
        # bs.append((net(img.to(device))).data.cpu())
    return torch.cat(bs).sign(), torch.cat(clses)

def compute_result_with_caption_txtangimg_all_in_hash(dataloader, net, device):
    bs, clses = [], []
    net.eval()
    for img, cls, _ in tqdm(dataloader):
        clses.append(cls)
        
        hash_codes, _, _, _, _ = net(img.to(device)) 
        bs.append(hash_codes.data.cpu())    
        # bs.append((net(img.to(device))).data.cpu())
    return torch.cat(bs).sign(), torch.cat(clses)

def CalcHammingDist(B1, B2):
    q = B2.shape[1]
    distH = 0.5 * (q - np.dot(B1, B2.transpose()))
    return distH


def CalcTopMap(rB, qB, retrievalL, queryL, topk):  # topk = -1
    num_query = queryL.shape[0]
    topkmap = 0
    for iter in tqdm(range(num_query)):
        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)
        hamm = CalcHammingDist(qB[iter, :], rB)
        ind = np.argsort(hamm)
        gnd = gnd[ind]

        tgnd = gnd[0:topk]
        # print(f"gnd.shape = {gnd.shape}------------------------") æ‰“å°å‡ºæ¥å°±æ˜¯æ£€ç´¢é›†æ€»å›¾ç‰‡æ•°é‡  CUBå°±æ˜¯5788
        tsum = np.sum(tgnd).astype(int)
        if tsum == 0:
            continue
        count = np.linspace(1, tsum, tsum)

        tindex = np.asarray(np.where(tgnd == 1)) + 1.0
        topkmap_ = np.mean(count / (tindex))
        topkmap = topkmap + topkmap_
    topkmap = topkmap / num_query
    return topkmap  # è¿”å› topkmap

